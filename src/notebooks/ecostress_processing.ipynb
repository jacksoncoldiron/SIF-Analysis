{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-00",
   "metadata": {},
   "source": [
    "# ECOSTRESS ET Processing Pipeline\n",
    "\n",
    "This notebook implements a complete QC and processing pipeline for ECOSTRESS L3T JET evapotranspiration tiles downloaded for Iowa (2019–2023). The output is a spatially aligned, cloud-masked, temporally organized stack of daily ET rasters ready for analysis.\n",
    "\n",
    "**Input:** Raw `ETdaily.tif` and `cloud.tif` GeoTIFFs in `data/raw/ECOSTRESS/`  \n",
    "**Output:** Processed rasters in `data/processed/ECOSTRESS/`\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **1** | **Verify Downloads** — confirm ETdaily + cloud layers present, check 2019–2023 temporal coverage |\n",
    "| **2** | **Apply QC Flags** — remove scenes obstructed by ISS solar panels or flagged for poor geolocation |\n",
    "| **3** | **Mosaic Tiles** — merge overlapping tiles per acquisition date, apply cloud mask + scale factor |\n",
    "| **4** | **Reproject & Clip** — reproject to WGS84, clip to Iowa AOI, resample to NLDAS 0.125° grid |\n",
    "| **5** | **Structure Temporally** — sort dates, export processed inventory CSV |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "**Libraries:**\n",
    "- `rasterio` / `rioxarray`: Raster I/O, reprojection, clipping\n",
    "- `rasterio.merge`: Mosaicking tiles per date\n",
    "- `geopandas`: Iowa AOI geometry\n",
    "- `numpy` / `pandas`: Array operations and tabular summaries\n",
    "- `pathlib`: Cross-platform file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rioxarray\n",
    "from rasterio.merge import merge\n",
    "from rasterio.mask import mask as rasterio_mask\n",
    "from rasterio.warp import Resampling, calculate_default_transform, reproject\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries loaded.\")\n",
    "print(f\"  rasterio: {rasterio.__version__}\")\n",
    "print(f\"  rioxarray: {rioxarray.__version__}\")\n",
    "print(f\"  geopandas: {gpd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Configuration\n",
    "\n",
    "All paths and processing parameters are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paths ──────────────────────────────────────────────────────────────────\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent  # SIF-Analysis/\n",
    "\n",
    "RAW_DIR        = PROJECT_ROOT / \"data\" / \"raw\" / \"ECOSTRESS\"\n",
    "AOI_PATH       = PROJECT_ROOT / \"data\" / \"aoi\" / \"iowa.geojson\"\n",
    "QC_OBST_FILE   = PROJECT_ROOT / \"src\" / \"scripts\" / \"obst_all_sort.txt\"\n",
    "QC_GEO_FILE    = PROJECT_ROOT / \"src\" / \"scripts\" / \"qa_20250423-present.txt\"\n",
    "\n",
    "OUT_MOSAIC_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"ECOSTRESS\" / \"mosaics\"\n",
    "OUT_CLIP_DIR   = PROJECT_ROOT / \"data\" / \"processed\" / \"ECOSTRESS\" / \"clipped\"\n",
    "\n",
    "OUT_MOSAIC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CLIP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ── QC Settings ────────────────────────────────────────────────────────────\n",
    "# Geolocation QA values to ACCEPT (reject 'Suspect' and 'NotFound')\n",
    "ACCEPT_GEOLOC_QA = {\"Best\", \"Good\"}\n",
    "\n",
    "# ── ECOSTRESS Scale Factor ─────────────────────────────────────────────────\n",
    "# ETdaily is stored as uint16; multiply by 0.1 to get mm/day\n",
    "ET_SCALE_FACTOR  = 0.1\n",
    "ET_NODATA_RAW    = 0      # raw NoData value before scaling\n",
    "ET_NODATA_SCALED = np.nan\n",
    "\n",
    "# ── Target CRS & Resolution ────────────────────────────────────────────────\n",
    "# WGS84 to match SIF (OCO-2) and NLDAS coordinate systems\n",
    "TARGET_CRS = \"EPSG:4326\"\n",
    "\n",
    "# NLDAS spatial resolution (degrees) for final resampling\n",
    "NLDAS_RES = 0.125\n",
    "\n",
    "# ── Study Period ───────────────────────────────────────────────────────────\n",
    "START_YEAR = 2019\n",
    "END_YEAR   = 2023\n",
    "\n",
    "print(f\"Project root:  {PROJECT_ROOT}\")\n",
    "print(f\"Raw data dir:  {RAW_DIR}  (exists: {RAW_DIR.exists()})\")\n",
    "print(f\"Iowa AOI:      {AOI_PATH}  (exists: {AOI_PATH.exists()})\")\n",
    "print(f\"Mosaic output: {OUT_MOSAIC_DIR}\")\n",
    "print(f\"Clipped output:{OUT_CLIP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-05",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1 — Verify Downloads\n",
    "\n",
    "Scan all TIFFs in `data/raw/ECOSTRESS/`, parse metadata from filenames, and verify that:\n",
    "- Both `ETdaily.tif` and `cloud.tif` are present for each scene\n",
    "- Coverage spans 2019–2023\n",
    "- No obvious gaps exist in the record\n",
    "\n",
    "**ECOSTRESS filename structure:**  \n",
    "`ECOv002_L3T_JET_{ORB}_{SCN}_{TILE}_{DATETIME}_{BUILD}_{VER}_{LAYER}.tif`\n",
    "- `ORB` = 5-digit orbit number  \n",
    "- `SCN` = 3-digit scene number  \n",
    "- `TILE` = MGRS tile ID (e.g., `15TVG`)  \n",
    "- `DATETIME` = `YYYYMMDDTHHMMSS`  \n",
    "- `LAYER` = `ETdaily` or `cloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ecostress_filename(path):\n",
    "    \"\"\"Parse metadata from an ECOSTRESS L3T JET filename.\n",
    "    \n",
    "    Returns a dict with keys: orbit, scene, tile, datetime, layer, stem.\n",
    "    Returns None if filename does not match expected pattern.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    parts = p.stem.split('_')  # strip .tif, then split\n",
    "    # Expected: ECOv002_L3T_JET_NNNNN_SSS_TILE_DATETIME_BUILD_VER_LAYER\n",
    "    if len(parts) < 10 or parts[0] != 'ECOv002':\n",
    "        return None\n",
    "    try:\n",
    "        orbit = int(parts[3])\n",
    "        scene = int(parts[4])\n",
    "        tile  = parts[5]\n",
    "        dt    = datetime.strptime(parts[6], \"%Y%m%dT%H%M%S\")\n",
    "        layer = parts[9]  # 'ETdaily' or 'cloud'\n",
    "        return {\n",
    "            'path':     p,\n",
    "            'orbit':    orbit,\n",
    "            'scene':    scene,\n",
    "            'tile':     tile,\n",
    "            'datetime': dt,\n",
    "            'date':     dt.date(),\n",
    "            'year':     dt.year,\n",
    "            'month':    dt.month,\n",
    "            'layer':    layer,\n",
    "            'stem':     p.stem,\n",
    "        }\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "# ── Scan all TIFFs ─────────────────────────────────────────────────────────\n",
    "all_tifs = sorted(RAW_DIR.glob(\"*.tif\"))\n",
    "print(f\"Total TIF files found: {len(all_tifs)}\")\n",
    "\n",
    "records = [parse_ecostress_filename(f) for f in all_tifs]\n",
    "records = [r for r in records if r is not None]\n",
    "df_all  = pd.DataFrame(records)\n",
    "\n",
    "et_df    = df_all[df_all['layer'] == 'ETdaily'].copy()\n",
    "cloud_df = df_all[df_all['layer'] == 'cloud'].copy()\n",
    "\n",
    "print(f\"  ETdaily files: {len(et_df)}\")\n",
    "print(f\"  cloud   files: {len(cloud_df)}\")\n",
    "print(f\"  Unrecognized:  {len(all_tifs) - len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Check ETdaily / cloud pairing ──────────────────────────────────────────\n",
    "# Build a scene key from orbit + scene number to check for matching pairs\n",
    "et_keys    = set(zip(et_df['orbit'],    et_df['scene'],    et_df['datetime']))\n",
    "cloud_keys = set(zip(cloud_df['orbit'], cloud_df['scene'], cloud_df['datetime']))\n",
    "\n",
    "paired    = et_keys & cloud_keys\n",
    "et_only   = et_keys - cloud_keys\n",
    "cloud_only= cloud_keys - et_keys\n",
    "\n",
    "print(\"Scene pairing summary:\")\n",
    "print(f\"  Fully paired (ETdaily + cloud): {len(paired):,}\")\n",
    "print(f\"  ETdaily only (no cloud file):   {len(et_only):,}\")\n",
    "print(f\"  cloud only   (no ETdaily):      {len(cloud_only):,}\")\n",
    "\n",
    "# ── Temporal coverage ──────────────────────────────────────────────────────\n",
    "print(\"\\nYear-by-year scene counts (ETdaily):\")\n",
    "year_counts = et_df.groupby('year').size()\n",
    "for yr, cnt in year_counts.items():\n",
    "    print(f\"  {yr}: {cnt:,} scenes\")\n",
    "\n",
    "missing_years = [y for y in range(START_YEAR, END_YEAR + 1) if y not in year_counts.index]\n",
    "if missing_years:\n",
    "    print(f\"\\nWARNING: No scenes found for years: {missing_years}\")\n",
    "else:\n",
    "    print(f\"\\nAll years {START_YEAR}–{END_YEAR} present.\")\n",
    "\n",
    "# ── Monthly heatmap ────────────────────────────────────────────────────────\n",
    "pivot = et_df.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "pivot = pivot.reindex(columns=range(1, 13), fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 3.5))\n",
    "im = ax.imshow(pivot.values, aspect='auto', cmap='YlGn', vmin=0)\n",
    "ax.set_xticks(range(12))\n",
    "ax.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun',\n",
    "                    'Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "ax.set_yticks(range(len(pivot)))\n",
    "ax.set_yticklabels(pivot.index)\n",
    "for r, year in enumerate(pivot.index):\n",
    "    for c, month in enumerate(range(1, 13)):\n",
    "        val = pivot.loc[year, month]\n",
    "        if val > 0:\n",
    "            ax.text(c, r, str(val), ha='center', va='center', fontsize=8)\n",
    "plt.colorbar(im, ax=ax, label='Scene count')\n",
    "ax.set_title('ECOSTRESS ETdaily Scene Count by Year and Month')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2 — Apply QC Flags\n",
    "\n",
    "Two QC issues can compromise ECOSTRESS data quality:\n",
    "\n",
    "1. **Solar panel obstruction (`obst_all_sort.txt`)** — The ISS solar panels can obstruct the ECOSTRESS field of view, producing corrupted thermal data. All entries in this file have `FOV_OBST=YES`. These scenes are excluded.\n",
    "\n",
    "2. **Geolocation accuracy** — Scenes with `GeolocationAccuracyQA = \"Suspect\"` or `\"NotFound\"` have unreliable pixel locations and are also excluded. This flag is embedded in `obst_all_sort.txt` for each obstructed scene, and in `qa_20250423-present.txt` for April 2025+ scenes.\n",
    "\n",
    "> **Note:** `qa_20250423-present.txt` covers data from April 23, 2025 onwards and does not apply to the 2019–2023 study period. It is parsed here for completeness.\n",
    "\n",
    "Scenes matching either flag are removed **before** mosaicking to avoid propagating bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_obstruction_file(path):\n",
    "    \"\"\"Parse obst_all_sort.txt.\n",
    "    \n",
    "    Returns:\n",
    "        bad_scenes  : set of (orbit, scene) — FOV obstructed, exclude all\n",
    "        suspect_geo : set of (orbit, scene) — also have bad geolocation\n",
    "    \"\"\"\n",
    "    bad_scenes  = set()\n",
    "    suspect_geo = set()\n",
    "\n",
    "    orb_re = re.compile(r'ORB=(\\d+)')\n",
    "    scn_re = re.compile(r'SCN=(\\d+)')\n",
    "    geo_re = re.compile(r'GeolocationAccuracyQA=\"([^\"]+)\"')\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            orb_m = orb_re.search(line)\n",
    "            scn_m = scn_re.search(line)\n",
    "            geo_m = geo_re.search(line)\n",
    "            if not (orb_m and scn_m):\n",
    "                continue\n",
    "            key = (int(orb_m.group(1)), int(scn_m.group(1)))\n",
    "            bad_scenes.add(key)\n",
    "            if geo_m and geo_m.group(1) not in ACCEPT_GEOLOC_QA:\n",
    "                suspect_geo.add(key)\n",
    "\n",
    "    return bad_scenes, suspect_geo\n",
    "\n",
    "\n",
    "def parse_geoloc_file(path):\n",
    "    \"\"\"Parse qa_20250423-present.txt.\n",
    "    \n",
    "    Returns set of (orbit, scene) with poor geolocation (Suspect/NotFound).\n",
    "    These reference L1B GEO files; orbit and scene are in the filename.\n",
    "    \"\"\"\n",
    "    suspect = set()\n",
    "    fn_re   = re.compile(r'ECOv002_L1B_GEO_(\\d+)_(\\d+)_')\n",
    "    geo_re  = re.compile(r'GeolocationAccuracyQA=\"([^\"]+)\"')\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            fn_m  = fn_re.search(line)\n",
    "            geo_m = geo_re.search(line)\n",
    "            if fn_m and geo_m and geo_m.group(1) not in ACCEPT_GEOLOC_QA:\n",
    "                key = (int(fn_m.group(1)), int(fn_m.group(2)))\n",
    "                suspect.add(key)\n",
    "\n",
    "    return suspect\n",
    "\n",
    "\n",
    "# ── Load QC flags ──────────────────────────────────────────────────────────\n",
    "bad_obstruction, suspect_geoloc_obst = parse_obstruction_file(QC_OBST_FILE)\n",
    "suspect_geoloc_geo = parse_geoloc_file(QC_GEO_FILE)\n",
    "\n",
    "# Union: all scenes to exclude\n",
    "# (1) solar panel obstruction — exclude all\n",
    "# (2) bad geolocation from either file\n",
    "all_bad_geoloc = suspect_geoloc_obst | suspect_geoloc_geo\n",
    "all_excluded   = bad_obstruction | all_bad_geoloc\n",
    "\n",
    "print(\"QC flag summary:\")\n",
    "print(f\"  Obstruction file:     {len(bad_obstruction):,} (orbit, scene) pairs flagged\")\n",
    "print(f\"    → of which, also bad geoloc: {len(suspect_geoloc_obst):,}\")\n",
    "print(f\"  Geoloc file (2025+): {len(suspect_geoloc_geo):,} pairs with poor geolocation\")\n",
    "print(f\"  Total unique excluded scenes:  {len(all_excluded):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Apply QC flags to ETdaily file list ────────────────────────────────────\n",
    "et_df['qc_key']        = list(zip(et_df['orbit'], et_df['scene']))\n",
    "et_df['obstructed']    = et_df['qc_key'].isin(bad_obstruction)\n",
    "et_df['bad_geoloc']    = et_df['qc_key'].isin(all_bad_geoloc)\n",
    "et_df['excluded']      = et_df['qc_key'].isin(all_excluded)\n",
    "\n",
    "et_pass = et_df[~et_df['excluded']].copy()\n",
    "et_fail = et_df[et_df['excluded']].copy()\n",
    "\n",
    "print(\"QC filtering results (ETdaily scenes):\")\n",
    "print(f\"  Total ETdaily scenes:     {len(et_df):,}\")\n",
    "print(f\"  Flagged — obstructed:     {et_df['obstructed'].sum():,}\")\n",
    "print(f\"  Flagged — bad geoloc:     {et_df['bad_geoloc'].sum():,}\")\n",
    "print(f\"  Total excluded:           {len(et_fail):,} ({100*len(et_fail)/len(et_df):.1f}%)\")\n",
    "print(f\"  Passed QC:                {len(et_pass):,} ({100*len(et_pass)/len(et_df):.1f}%)\")\n",
    "\n",
    "# Year-by-year breakdown after QC\n",
    "print(\"\\nScenes passing QC per year:\")\n",
    "for yr, cnt in et_pass.groupby('year').size().items():\n",
    "    total_yr = len(et_df[et_df['year'] == yr])\n",
    "    print(f\"  {yr}: {cnt:,} / {total_yr:,} ({100*cnt/total_yr:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3 — Mosaic Tiles by Acquisition Date\n",
    "\n",
    "Multiple ECOSTRESS tiles may cover Iowa on the same acquisition pass. This step:\n",
    "\n",
    "1. Groups QC-passed ETdaily files by **date** (`YYYYMMDD`)\n",
    "2. For each scene, applies the paired cloud mask (pixels with cloud value > 0 are set to NaN)\n",
    "3. Applies the **scale factor** (×0.1) to convert raw uint16 to mm/day\n",
    "4. Merges all tiles for that date into a single mosaicked GeoTIFF\n",
    "5. Saves to `data/processed/ECOSTRESS/mosaics/`\n",
    "\n",
    "> **Cloud mask values:** By default, pixels with cloud mask > 0 are treated as cloudy. The first available cloud file is inspected to confirm the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inspect cloud mask encoding ────────────────────────────────────────────\n",
    "sample_cloud = cloud_df['path'].iloc[0] if len(cloud_df) > 0 else None\n",
    "\n",
    "if sample_cloud and sample_cloud.exists():\n",
    "    with rasterio.open(sample_cloud) as src:\n",
    "        cloud_sample = src.read(1)\n",
    "        unique_vals  = np.unique(cloud_sample)\n",
    "    print(f\"Cloud mask sample: {sample_cloud.name}\")\n",
    "    print(f\"  Unique values: {unique_vals}\")\n",
    "    print(f\"  dtype: {cloud_sample.dtype}\")\n",
    "    print(\"  Interpretation: values > 0 will be treated as cloudy (masked).\")\n",
    "else:\n",
    "    print(\"No cloud files found — cloud masking will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build cloud file lookup: (orbit, scene, datetime) -> cloud path ─────────\n",
    "cloud_lookup = {\n",
    "    (r['orbit'], r['scene'], r['datetime']): r['path']\n",
    "    for _, r in cloud_df.iterrows()\n",
    "}\n",
    "\n",
    "\n",
    "def load_et_with_cloud_mask(et_path, cloud_path=None):\n",
    "    \"\"\"Read ETdaily TIF, apply cloud mask, apply scale factor.\n",
    "    \n",
    "    Returns:\n",
    "        data    : float32 array, mm/day, NaN where cloudy or NoData\n",
    "        profile : rasterio profile of the source file\n",
    "    \"\"\"\n",
    "    with rasterio.open(et_path) as src:\n",
    "        raw     = src.read(1).astype(np.float32)\n",
    "        profile = src.profile.copy()\n",
    "\n",
    "    # Apply scale factor and NoData mask\n",
    "    data = raw * ET_SCALE_FACTOR\n",
    "    data[raw == ET_NODATA_RAW] = np.nan\n",
    "\n",
    "    # Apply cloud mask if available\n",
    "    if cloud_path is not None and Path(cloud_path).exists():\n",
    "        with rasterio.open(cloud_path) as csrc:\n",
    "            cloud = csrc.read(1)\n",
    "        data[cloud > 0] = np.nan\n",
    "\n",
    "    profile.update(dtype='float32', nodata=np.nan, count=1)\n",
    "    return data, profile\n",
    "\n",
    "\n",
    "def mosaic_date(date_rows, cloud_lookup, out_dir):\n",
    "    \"\"\"Mosaic all QC-passed tiles for a single acquisition date.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date_rows    : DataFrame rows for one date\n",
    "    cloud_lookup : dict mapping (orbit, scene, datetime) to cloud TIF path\n",
    "    out_dir      : output directory for mosaic TIFs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out_path : Path to saved mosaic, or None if all tiles were empty\n",
    "    \"\"\"\n",
    "    date_str  = date_rows.iloc[0]['date'].strftime(\"%Y%m%d\")\n",
    "    out_path  = out_dir / f\"ECOSTRESS_ET_{date_str}.tif\"\n",
    "\n",
    "    if out_path.exists():\n",
    "        return out_path  # skip already-processed dates\n",
    "\n",
    "    mem_files = []\n",
    "    tmp_paths = []\n",
    "\n",
    "    for _, row in date_rows.iterrows():\n",
    "        cloud_key  = (row['orbit'], row['scene'], row['datetime'])\n",
    "        cloud_path = cloud_lookup.get(cloud_key)\n",
    "\n",
    "        data, profile = load_et_with_cloud_mask(row['path'], cloud_path)\n",
    "\n",
    "        # Skip if tile is entirely NaN\n",
    "        if np.all(np.isnan(data)):\n",
    "            continue\n",
    "\n",
    "        # Write to a temp in-memory file for merging\n",
    "        tmp = rasterio.MemoryFile()\n",
    "        with tmp.open(**profile) as dst:\n",
    "            dst.write(data[np.newaxis, :, :])\n",
    "        mem_files.append(tmp)\n",
    "        tmp_paths.append(tmp)\n",
    "\n",
    "    if not mem_files:\n",
    "        return None  # all tiles empty after masking\n",
    "\n",
    "    # Open the in-memory files for merging\n",
    "    opened = [mf.open() for mf in mem_files]\n",
    "    mosaic, out_transform = merge(opened, method='first', nodata=np.nan)\n",
    "    out_profile = opened[0].profile.copy()\n",
    "    out_profile.update(\n",
    "        height=mosaic.shape[1],\n",
    "        width=mosaic.shape[2],\n",
    "        transform=out_transform,\n",
    "        compress='lzw'\n",
    "    )\n",
    "\n",
    "    with rasterio.open(out_path, 'w', **out_profile) as dst:\n",
    "        dst.write(mosaic)\n",
    "\n",
    "    for ds in opened:\n",
    "        ds.close()\n",
    "    for mf in mem_files:\n",
    "        mf.close()\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "print(\"Mosaic functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run mosaicking for all acquisition dates ───────────────────────────────\n",
    "# Group QC-passed ETdaily scenes by date\n",
    "grouped = et_pass.groupby('date')\n",
    "dates   = sorted(grouped.groups.keys())\n",
    "\n",
    "print(f\"Processing {len(dates)} unique acquisition dates...\")\n",
    "print(\"(Dates already mosaicked will be skipped.)\\n\")\n",
    "\n",
    "mosaic_log = []  # track results\n",
    "\n",
    "for i, date in enumerate(dates):\n",
    "    date_rows = grouped.get_group(date)\n",
    "    out_path  = mosaic_date(date_rows, cloud_lookup, OUT_MOSAIC_DIR)\n",
    "\n",
    "    mosaic_log.append({\n",
    "        'date':       date,\n",
    "        'n_tiles':    len(date_rows),\n",
    "        'mosaicked':  out_path is not None,\n",
    "        'path':       str(out_path) if out_path else None,\n",
    "    })\n",
    "\n",
    "    if (i + 1) % 100 == 0 or (i + 1) == len(dates):\n",
    "        done = sum(r['mosaicked'] for r in mosaic_log)\n",
    "        print(f\"  [{i+1}/{len(dates)}] {done} mosaics written, \"\n",
    "              f\"{len(mosaic_log)-done} empty/skipped\")\n",
    "\n",
    "mosaic_df = pd.DataFrame(mosaic_log)\n",
    "print(f\"\\nMosaic complete.\")\n",
    "print(f\"  Total dates:    {len(mosaic_df)}\")\n",
    "print(f\"  Mosaics saved:  {mosaic_df['mosaicked'].sum()}\")\n",
    "print(f\"  Empty (all NaN):{(~mosaic_df['mosaicked']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4 — Reproject, Clip, and Resample to Iowa / NLDAS Grid\n",
    "\n",
    "ECOSTRESS tiles use UTM projections (multiple zones across Iowa). This step:\n",
    "\n",
    "1. **Reprojects** each mosaic to **WGS84 (EPSG:4326)** — the CRS of the SIF (OCO-2) and NLDAS datasets\n",
    "2. **Clips** to the Iowa state boundary (`data/aoi/iowa.geojson`)\n",
    "3. **Resamples** from ECOSTRESS native resolution (~70m) to **NLDAS 0.125°** using area-averaging (`Resampling.average`), enabling direct comparison with NLDAS ET\n",
    "\n",
    "Output rasters are saved to `data/processed/ECOSTRESS/clipped/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load Iowa AOI ──────────────────────────────────────────────────────────\n",
    "iowa_gdf = gpd.read_file(AOI_PATH)\n",
    "\n",
    "# Ensure it's in WGS84 for clipping and grid alignment\n",
    "iowa_gdf = iowa_gdf.to_crs(TARGET_CRS)\n",
    "iowa_bounds = iowa_gdf.total_bounds  # (minx, miny, maxx, maxy)\n",
    "\n",
    "print(f\"Iowa AOI loaded: {len(iowa_gdf)} feature(s)\")\n",
    "print(f\"  CRS:    {iowa_gdf.crs}\")\n",
    "print(f\"  Bounds: W={iowa_bounds[0]:.3f} S={iowa_bounds[1]:.3f} \"\n",
    "      f\"E={iowa_bounds[2]:.3f} N={iowa_bounds[3]:.3f}\")\n",
    "\n",
    "# ── Define NLDAS-aligned output grid ──────────────────────────────────────\n",
    "# Snap Iowa bounds to the nearest 0.125° NLDAS grid cell\n",
    "res = NLDAS_RES\n",
    "grid_west  = np.floor(iowa_bounds[0] / res) * res\n",
    "grid_south = np.floor(iowa_bounds[1] / res) * res\n",
    "grid_east  = np.ceil( iowa_bounds[2] / res) * res\n",
    "grid_north = np.ceil( iowa_bounds[3] / res) * res\n",
    "\n",
    "grid_width  = int(round((grid_east  - grid_west)  / res))\n",
    "grid_height = int(round((grid_north - grid_south) / res))\n",
    "grid_transform = from_bounds(grid_west, grid_south, grid_east, grid_north,\n",
    "                              grid_width, grid_height)\n",
    "\n",
    "print(f\"\\nNLDAS-aligned output grid:\")\n",
    "print(f\"  Resolution: {res}°\")\n",
    "print(f\"  Extent:     W={grid_west} S={grid_south} E={grid_east} N={grid_north}\")\n",
    "print(f\"  Size:       {grid_width} cols × {grid_height} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Reproject + clip + resample function ───────────────────────────────────\n",
    "iowa_shapes = [geom.__geo_interface__ for geom in iowa_gdf.geometry]\n",
    "\n",
    "\n",
    "def reproject_clip_resample(mosaic_path, out_dir):\n",
    "    \"\"\"Reproject mosaic to WGS84, clip to Iowa, resample to NLDAS grid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mosaic_path : Path to input mosaic TIF (UTM projection)\n",
    "    out_dir     : Output directory\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out_path : Path to saved clipped TIF, or None on error\n",
    "    \"\"\"\n",
    "    out_path = out_dir / mosaic_path.name\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    try:\n",
    "        # Step 1: Reproject UTM → WGS84 at native resolution\n",
    "        with rasterio.open(mosaic_path) as src:\n",
    "            if src.crs is None:\n",
    "                return None\n",
    "\n",
    "            # Calculate intermediate WGS84 transform (native res)\n",
    "            wgs_transform, wgs_width, wgs_height = calculate_default_transform(\n",
    "                src.crs, TARGET_CRS, src.width, src.height, *src.bounds\n",
    "            )\n",
    "            wgs_data = np.full((wgs_height, wgs_width), np.nan, dtype=np.float32)\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=wgs_data,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=wgs_transform,\n",
    "                dst_crs=TARGET_CRS,\n",
    "                resampling=Resampling.bilinear,\n",
    "                src_nodata=np.nan,\n",
    "                dst_nodata=np.nan,\n",
    "            )\n",
    "            wgs_profile = src.profile.copy()\n",
    "            wgs_profile.update(\n",
    "                crs=TARGET_CRS,\n",
    "                transform=wgs_transform,\n",
    "                width=wgs_width,\n",
    "                height=wgs_height,\n",
    "                dtype='float32',\n",
    "                nodata=np.nan,\n",
    "            )\n",
    "\n",
    "        # Step 2: Resample to NLDAS 0.125° grid using area averaging\n",
    "        nldas_data = np.full((grid_height, grid_width), np.nan, dtype=np.float32)\n",
    "        with rasterio.MemoryFile() as memfile:\n",
    "            with memfile.open(**wgs_profile) as tmp:\n",
    "                tmp.write(wgs_data[np.newaxis, :, :])\n",
    "            with memfile.open() as tmp:\n",
    "                reproject(\n",
    "                    source=rasterio.band(tmp, 1),\n",
    "                    destination=nldas_data,\n",
    "                    src_transform=wgs_transform,\n",
    "                    src_crs=TARGET_CRS,\n",
    "                    dst_transform=grid_transform,\n",
    "                    dst_crs=TARGET_CRS,\n",
    "                    resampling=Resampling.average,\n",
    "                    src_nodata=np.nan,\n",
    "                    dst_nodata=np.nan,\n",
    "                )\n",
    "\n",
    "        # Step 3: Clip to Iowa state boundary\n",
    "        nldas_profile = wgs_profile.copy()\n",
    "        nldas_profile.update(\n",
    "            transform=grid_transform,\n",
    "            width=grid_width,\n",
    "            height=grid_height,\n",
    "        )\n",
    "        with rasterio.MemoryFile() as memfile:\n",
    "            with memfile.open(**nldas_profile) as tmp:\n",
    "                tmp.write(nldas_data[np.newaxis, :, :])\n",
    "            with memfile.open() as tmp:\n",
    "                clipped, clip_transform = rasterio_mask(\n",
    "                    tmp, iowa_shapes, crop=True, nodata=np.nan, filled=True\n",
    "                )\n",
    "                clip_profile = tmp.profile.copy()\n",
    "                clip_profile.update(\n",
    "                    transform=clip_transform,\n",
    "                    width=clipped.shape[2],\n",
    "                    height=clipped.shape[1],\n",
    "                    compress='lzw',\n",
    "                )\n",
    "\n",
    "        with rasterio.open(out_path, 'w', **clip_profile) as dst:\n",
    "            dst.write(clipped)\n",
    "\n",
    "        return out_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing {mosaic_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Reproject/clip function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run reproject + clip for all mosaics ──────────────────────────────────\n",
    "mosaic_files = sorted(OUT_MOSAIC_DIR.glob(\"ECOSTRESS_ET_*.tif\"))\n",
    "print(f\"Reprojecting and clipping {len(mosaic_files)} mosaics...\")\n",
    "print(\"(Already processed files will be skipped.)\\n\")\n",
    "\n",
    "clip_log = []\n",
    "\n",
    "for i, mosaic_path in enumerate(mosaic_files):\n",
    "    out_path = reproject_clip_resample(mosaic_path, OUT_CLIP_DIR)\n",
    "    clip_log.append({\n",
    "        'mosaic': mosaic_path.name,\n",
    "        'clipped': out_path is not None,\n",
    "        'path':    str(out_path) if out_path else None,\n",
    "    })\n",
    "\n",
    "    if (i + 1) % 100 == 0 or (i + 1) == len(mosaic_files):\n",
    "        done = sum(r['clipped'] for r in clip_log)\n",
    "        print(f\"  [{i+1}/{len(mosaic_files)}] {done} files written\")\n",
    "\n",
    "clip_df = pd.DataFrame(clip_log)\n",
    "print(f\"\\nReproject/clip complete.\")\n",
    "print(f\"  Processed: {clip_df['clipped'].sum()}\")\n",
    "print(f\"  Errors:    {(~clip_df['clipped']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5 — Temporal Structure and Inventory\n",
    "\n",
    "Build a complete inventory of processed ECOSTRESS files: one row per date with file path, year, month, day-of-year, and valid pixel coverage. Export as CSV for use in downstream analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build temporal inventory ───────────────────────────────────────────────\n",
    "clipped_files = sorted(OUT_CLIP_DIR.glob(\"ECOSTRESS_ET_*.tif\"))\n",
    "print(f\"Building inventory from {len(clipped_files)} clipped files...\")\n",
    "\n",
    "inventory = []\n",
    "for f in clipped_files:\n",
    "    # Parse date from filename: ECOSTRESS_ET_YYYYMMDD.tif\n",
    "    date_str = f.stem.split('_')[-1]  # 'YYYYMMDD'\n",
    "    try:\n",
    "        dt = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Quick valid-pixel check\n",
    "    try:\n",
    "        with rasterio.open(f) as src:\n",
    "            data = src.read(1)\n",
    "            total  = data.size\n",
    "            valid  = int(np.sum(~np.isnan(data.astype(float))))\n",
    "            mean_et = float(np.nanmean(data)) if valid > 0 else np.nan\n",
    "    except Exception:\n",
    "        total, valid, mean_et = 0, 0, np.nan\n",
    "\n",
    "    inventory.append({\n",
    "        'date':          dt.date(),\n",
    "        'year':          dt.year,\n",
    "        'month':         dt.month,\n",
    "        'doy':           dt.timetuple().tm_yday,\n",
    "        'valid_pixels':  valid,\n",
    "        'total_pixels':  total,\n",
    "        'coverage_pct':  round(100 * valid / total, 1) if total > 0 else 0,\n",
    "        'mean_et_mm_day': round(mean_et, 3),\n",
    "        'path':          str(f),\n",
    "    })\n",
    "\n",
    "inv_df = pd.DataFrame(inventory).sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Save inventory CSV\n",
    "inv_path = PROJECT_ROOT / \"data\" / \"processed\" / \"ECOSTRESS\" / \"ecostress_inventory.csv\"\n",
    "inv_df.to_csv(inv_path, index=False)\n",
    "\n",
    "print(f\"\\nInventory saved to: {inv_path}\")\n",
    "print(f\"  Total processed dates: {len(inv_df)}\")\n",
    "print(f\"  Date range: {inv_df['date'].min()} to {inv_df['date'].max()}\")\n",
    "print(f\"  Mean coverage: {inv_df['coverage_pct'].mean():.1f}%\")\n",
    "print(f\"  Mean ET (Iowa daily): {inv_df['mean_et_mm_day'].mean():.3f} mm/day\")\n",
    "print()\n",
    "print(\"Scenes per year:\")\n",
    "for yr, cnt in inv_df.groupby('year').size().items():\n",
    "    print(f\"  {yr}: {cnt} dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Summary plots ──────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(2, 1, figsize=(13, 8))\n",
    "\n",
    "# Top: scene count per month across all years\n",
    "monthly = inv_df.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "monthly = monthly.reindex(columns=range(1, 13), fill_value=0)\n",
    "im = axes[0].imshow(monthly.values, aspect='auto', cmap='YlGn', vmin=0)\n",
    "axes[0].set_xticks(range(12))\n",
    "axes[0].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun',\n",
    "                          'Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "axes[0].set_yticks(range(len(monthly)))\n",
    "axes[0].set_yticklabels(monthly.index)\n",
    "for r, yr in enumerate(monthly.index):\n",
    "    for c, mo in enumerate(range(1, 13)):\n",
    "        v = monthly.loc[yr, mo]\n",
    "        if v > 0:\n",
    "            axes[0].text(c, r, str(v), ha='center', va='center', fontsize=8)\n",
    "plt.colorbar(im, ax=axes[0], label='# dates')\n",
    "axes[0].set_title('Processed ECOSTRESS Dates per Month (after QC + mosaicking)')\n",
    "\n",
    "# Bottom: mean daily ET time series\n",
    "for yr, grp in inv_df.groupby('year'):\n",
    "    axes[1].plot(grp['doy'], grp['mean_et_mm_day'], '.', alpha=0.4,\n",
    "                 markersize=3, label=str(yr))\n",
    "axes[1].set_xlabel('Day of Year')\n",
    "axes[1].set_ylabel('Mean ET (mm/day)')\n",
    "axes[1].set_title('Iowa Mean Daily ET — Processed ECOSTRESS (2019–2023)')\n",
    "axes[1].legend(title='Year', loc='upper left', ncol=5)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_dir = PROJECT_ROOT / \"figures\" / \"ecostress\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"ecostress_processing_summary.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Summary figure saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pipeline Summary\n",
    "\n",
    "| Output | Location |\n",
    "|--------|----------|\n",
    "| Cloud-masked, mosaicked daily TIFs (UTM) | `data/processed/ECOSTRESS/mosaics/` |\n",
    "| Reprojected + clipped TIFs (WGS84, NLDAS grid) | `data/processed/ECOSTRESS/clipped/` |\n",
    "| Processed date inventory CSV | `data/processed/ECOSTRESS/ecostress_inventory.csv` |\n",
    "| Summary figure | `figures/ecostress/ecostress_processing_summary.png` |\n",
    "\n",
    "**Next step:** Use `data/processed/ECOSTRESS/clipped/` in the irrigation signal analysis alongside NLDAS modeled ET to compute the PET – ET residual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
